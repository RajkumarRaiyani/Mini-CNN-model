# GROUP-4 Representing and Scheduling of neural network
# OBJECTIVE 
The main goal of this project is to _**represent**_ and _**schedule**_ the neural networks.
1. In order to represent, First we have taken the pre-trained neural network and export it into ONNX format. By which we have enabled our network to 
   adapt to any different hardware platforms. Thus improving the interpretability and explainability of the network. 
2. To apply a neural network on actual data, memory needs to be allocated for each operator and network operators need to be executed in the correct 
   order. The Scheduler can optimize the graph to select different implementation variants for individual part of the network graph.

# **NOTE**
   >While the Implementation of CNN operators and important building blocks of CNNs is done 
    by another group of this project.
# Overview of Lenet5 model Architecture
*    First of all, we have to understand the pre-trained model that we have taken from github. Here our pre-trained model is one of the very earliest with 
   simple architecture Lenet-5. In which, We have 2 set of Convolutional layer with Pooling layer. And 3 layers of FC(Fully connected layers).These all 
   layers and its operators are created by another group. From netron tool we can verify that the height and width of our  image over the layer changes 
   like ( 28 - 24 - 12 - 8 - 4 ). So here we have 256 inputs at the end of second average pooling layer. And we have total 120,84,10 neurons in our three 
   respective Fully Connected layers.
*  Then, We have converted this model into ONNX format using the code no.2. in the code section of this project. Using function like onnxml or utility 
   functions of onnxml. Second method with onnxmltools.convert_keras is way easier than the first method. After successfully converting into onnx format 
   we have to understand the new terms that is introduced by conversion of file into onnx format.
*  In order to understand that we can visualize this .onnx type file into Netron tool which is a tool that visualizes the attributes and operator type
   connected to each layer and inputs and outputs of each layer with its shape. And the netron pipeline of a model we can see from 6th line of code section of the 
   project.
# Understanding model with their protos
*  Basically it's a heirarchy of the protos. As in the below figure of flowchart shown, on top of that heirarchy "modelproto" comes which means our full model with all the information.
   In which there is main dictionary called 'graph', Which is also called as "graphproto". In which there are three further subtypes of dictionaries. All the
   attributes 
   like stride,pooling,pooling size,and operator type are contained in 'attributeproto' which data is available in dictionary called 'Node'. Then all the information
   regarding inputs/outputs are stored in the "valueinfoproto" which is again in the 'node' dictionary. And the last we have "Tensorproto" Which contains all the 
   Initializers information like list of weights of operators,kernel and raw data.And it is stored in dictionary called  'INITIALIZER'.
*  So now we will focus on how to extract all the information that is needed for implementing the neural network. So now we know that all the information are in two 
   dictionaries called, Node and Initializer. So for that we have used some empty lists and store the value of all the op_type or attribute or weights according to the
   order of the op_type.
*  As we can see from the code(No.3 file in the code section) that for extracting we need to just see at our modelfile and according to that we can store the values
   to assigned variables. But here our model is small so according our model we can extract data manually but in case of big and complex network we can not do it
   manually. And this was the most challenging part of our project to encapsulate all these value in a function and zipping the op_type name and beside it,its value in
   array format. with the help of onnx_helper we can convert the raw data contained in initializer to 4D array. And than we got our values ready in a dictionary zipped
   by its name and value.
*  This way we extracted our data for implementing it to the neural network. From code no.3 we can get data manually and from code no.4 we can get data using function.
*  And below is the list of data required to connect with implementation side of the project.
# List of data 
  
   1.    Operator list in sequence for lenet-5
   2.    Values for the all Kernels of Convolution operation 
   3.    Padding and stride values of Convolution operation 
   4.    Values for the all Biases of Convolution operation
   5.    Type of Activation functions 
   6.    A Value of Constant for Activation functions if required 
   7.    Type of Pooling operation
   8.    Pooling Size and Stride of Pooling operation
   9.    Values for the all Weights of FCC operation
   10.   Values for the all Biases of FCC operation
   11.   Type of Bias(tied or untied)
   
   
##  FLOWCHART OF PROTO


   ![Flowchart](https://user-images.githubusercontent.com/116641166/213934415-273ded55-6a9d-4dc7-9d3e-0b603e09c2f4.png)

   
